# -*- coding: utf-8 -*-
"""Non-linear Regression Approaches.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VDZNHggsXAngqsZ7XGCbclhahEzOWSmm
"""



"""# Tensorflow Linear Regression

# Non-Linear Regression using low-level API
"""

# Generating random data to work on

true_a, true_b, true_c = 1, -3,7
noise_mean = 0
noise_std = 10 # noise_std must be higher than 1, as it implies the data distribuetd from the mean
samples = 1000

def numpy_data():
  x = np.linspace(-10, 10, samples)
  y_true = true_a*(x**2) + true_b*x + true_c
  noise = np.random.normal(noise_mean, noise_std, samples)
  y_noisy = y_true + noise
  return x, y_true, y_noisy

def tf_data():
  x = tf.linspace(-10.0,10.0,samples) # start, stop must be floats
  y_true = true_a*(tf.square(x)) + true_b*(x) + true_c  # tf.square() to get a tensor instead of x**2
  noise = tf.random.normal(mean=noise_mean, stddev=noise_std, shape=[samples]) # "[]" is a must as it mentions the shape of the output
  y_noisy = y_true + noise
  return x, y_true, y_noisy

x,y_true,y_noisy = tf_data()

# Plot the data points
plt.scatter(x, y_noisy, color='green',label='Data Points') # Scatter plot is made on noised data to have the data point scattered
plt.plot(x, y_true, color='blue', label='Quadratic Curve', linewidth=4) # Line plot is made on the equation true data to have the data points in a straight line
plt.xlabel('x')
plt.ylabel('y')
plt.title('Scatter Plot with Quadratic Curve')
plt.legend()
plt.show()

# Know if the data is tensor or numpy
print(type(x),"\n", type(y_true),"\n", type(y_noisy))

# define the model

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Defining the tf low-level API model
class Tf_regression(tf.Module): # tf.Module instead of tf.keras.Model
  def __init__(self): # No need of super initialization
    # defining the tensors/ weights that are updated in the model training
    self.a = tf.Variable(np.random.randn(), dtype=tf.float32)
    self.b = tf.Variable(np.random.randn(), dtype=tf.float32)
    self.c = tf.Variable(np.random.randn(), dtype=tf.float32) # this is ✅ as tf.Variable is mutable
    # self.a = tf.convert_to_tensor(np.random.randn(), dtype=tf.float32) # this is ❌ as tf.convert_to_tensor is not mutable


  def __call__(self, x): # __call__ instead of call as the tf.Module has to be callable
    # defnining the output of the model. This value is compared with the original and loss function is calculated
    y = self.a*(x**2) + self.b*x + self.c
    return y

# Compiling montage

model = Tf_regression()

optimizer = tf.optimizers.Adam(learning_rate=0.1) # if the loss is decreasing in very slow rate, increase the learning rate
# optimizer = tf.optimizers.SGD(learning_rate=0.01)

def loss_function(y_true, y_pred):
  return tf.reduce_mean(tf.square(y_true - y_pred))

def train_step(x,y):
  with tf.GradientTape() as tape:
    y_pred = model(x)
    loss = loss_function(y, y_pred)
  gradient = tape.gradient(loss, model.trainable_variables)
  # or gradient = tape.gradient(loss, [model.a, model.b, model.c])
  optimizer.apply_gradients(zip(gradient, model.trainable_variables)) # the zip corresponds teh gradient to each of the trainable
  return loss

def train(x,y,epochs):
  for epoch in range(epochs):
    loss = train_step(x,y)
    print("At Epoch: {0} the loss found is: {1}".format(epoch, loss))

# Before Training
plt.scatter(x,y_noisy, label="Noisy Data", c="green")
plt.plot(x,y_true, label="True Data", c="blue", linewidth=3)
plt.plot(x, model(x), label="Model Prediction", c="red", linewidth=3)
plt.legend()
plt.show()

# Calling the model and set params
print("Initial weights assigned: ",model.a.numpy(), model.b.numpy(), model.c.numpy())
print("True weights: ", true_a, true_b, true_c)

# Training

epoch = 500
train(x,y_noisy,epoch)

# After training

plt.scatter(x,y_noisy, label="Noisy Data", c="green")
plt.plot(x,y_true, label="True Data", c="blue", linewidth=3)
plt.plot(x, model(x), label="Model Prediction", c="red", linewidth=3)
plt.legend()
plt.show()

# Calling the model and set params
print("Final weights assigned: ",model.a.numpy(), model.b.numpy(), model.c.numpy())
print("True weights: ", true_a, true_b, true_c)

"""# Non-Linear Regression using high-level API"""

# Generating random data to work on

true_a, true_b, true_c = 1, -3,7
noise_mean = 0
noise_std = 10 # noise_std must be higher than 1, as it implies the data distribuetd from the mean
samples = 1000

def numpy_data():
  x = np.linspace(-10, 10, samples)
  y_true = true_a*(x**2) + true_b*x + true_c
  noise = np.random.normal(noise_mean, noise_std, samples)
  y_noisy = y_true + noise
  return x, y_true, y_noisy

def tf_data():
  x = tf.linspace(-10.0,10.0,samples) # start, stop must be floats
  y_true = true_a*(tf.square(x)) + true_b*(x) + true_c  # tf.square() to get a tensor instead of x**2
  noise = tf.random.normal(mean=noise_mean, stddev=noise_std, shape=[samples]) # "[]" is a must as it mentions the shape of the output
  y_noisy = y_true + noise
  return x, y_true, y_noisy

x,y_true,y_noisy = numpy_data()

# Plot the data points
plt.scatter(x, y_noisy, color='green',label='Data Points') # Scatter plot is made on noised data to have the data point scattered
plt.plot(x, y_true, color='blue', label='Quadratic Curve', linewidth=4) # Line plot is made on the equation true data to have the data points in a straight line
plt.xlabel('x')
plt.ylabel('y')
plt.title('Scatter Plot with Quadratic Curve')
plt.legend()
plt.show()

# Know if the data is tensor or numpy
print(type(x),"\n", type(y_true),"\n", type(y_noisy))

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Defining the tf high-level API model
class Kr_regression(tf.keras.Model): # tf.Module
  def __init__(self):
    super(Kr_regression, self).__init__() # NA
    self.a = self.add_weight(shape=(), trainable=True, initializer="random_normal") # tf.Variable(np.random.randn(), dtype=tf.float32)
    self.b = self.add_weight(shape=(), trainable=True, initializer="random_normal")
    self.c = self.add_weight(shape=(), trainable=True, initializer="random_normal")

  def call(self, x): # __call__
    y = self.a*(x**2) + self.b*x + self.c
    return y

"""
Note
1. shape=(): Defines the dimensions of the weight.
  scalar parameters = empty tuple () - meaning this weight is a single value ~ randn()
  vector parameter = (10,) ~ randn(10)
  matrix = (10,10) ~ randn(10,10)
2. initializer = Determines how the weight tensor is initialized
  random_normal: Initializes the weights with random values from a normal distribution (Guassian)
  other: 'zeros', 'ones', 'glorot_uniform', 'he_normal'
3. trainable: Specifies whether the weight is trainable or not.
  True, False

initializer='random_normal': Sets the initial values of the weight. 'random_normal' initializes with values from a normal distribution.
trainable=True: Indicates whether this weight should be updated during training.
"""

# Compilation montage
model = Kr_regression()

optimizer= tf.keras.optimizers.Adam(learning_rate=0.001) # instead of tf.optimizers.Adam, its tf.keras.optimizers.Adam
loss = "mean_squared_error" # instead of "mse"

model.compile(optimizer=optimizer, loss=loss)

# Before training

plt.scatter(x, y_noisy, c="green", label="Noisy Data")
plt.plot(x, y_true, c="blue", label="True Data")
plt.plot(x, model(x), c="red", label="Model Prediction")
plt.legend()
plt.show()

model.fit(x, y_noisy, epochs=200, verbose=2)

# After training

plt.scatter(x, y_noisy, c="green", label="Noisy Data")
plt.plot(x, y_true, c="blue", label="True Data")
plt.plot(x, model(x), c="red", label="Model Prediction")
plt.legend()
plt.show()

"""# Non-Linear Regression using high level API with Dense layers

"""

# generating sample data

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

true_a, true_b, true_c = 1,-3,7
def numpy_data(num):
  x = np.linspace(-10,10,num)
  y_true = true_a*(x**2) + true_b*(x) + true_c
  noise = np.random.normal(0,10,num)
  y_noisy = y_true + noise
  return x,y_true,y_noisy

def tf_data(num):
  x = tf.linspace(-10,10,num)
  y_true = true_a*tf.square(x) + true_b*x + true_c
  noise = tf.random.normal(mean=0, stddev=10, shape=[num])
  y_noisy = y_true + noise
  return x,y_true,y_noisy

x, y_true, y_noise = numpy_data(1000)
# plot the data
plt.scatter(x, y_noise, c="red", label="noisy data")
plt.plot(x, y_true, c="blue", label="true data")
plt.legend()
plt.show()

# Model subclass architecture
import tensorflow as tf
from tensorflow.keras.layers import Dense

class Kr_layer_regression(tf.keras.Model):
  def __init__(self):
    super(Kr_layer_regression, self).__init__()
    self.dense = Dense(10, activation="relu")
    self.output_layer = Dense(1)

  def call(self, inputs):
    x = self.dense(inputs)
    return self.output_layer(x)

  def build(self, input_shape):
    super(Kr_layer_regression, self).build(input_shape)
    self.dense.build(input_shape)
    dense_output_shape = self.dense.compute_output_shape(input_shape)
    self.output_layer.build(dense_output_shape)
model = Kr_layer_regression()


# Model sequential architecture
import tensorflow as tf
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.models import Sequential

model_2 = Sequential()
# model_2.add(InputLayer(shape=(1,)))
model_2.add(Dense(10, activation='relu', input_shape=(1,)))  # Specify input_shape here
model_2.add(Dense(1))




# Model Functional architetcure
from tensorflow.keras.layers import Input
inputs = Input(shape=(1,))
x = Dense(10, activation="relu")(inputs)
x = Dense(1)(x)
model_3 = tf.keras.Model(inputs=inputs, outputs=x)

print(model.summary())
print(model_2.summary())
print(model_3.summary())

model.build((None, 1))

print(model.summary())
print(model_2.summary())
print(model_3.summary())

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss = "mean_squared_error"

model.compile(optimizer=optimizer, loss=loss)
model_2.compile(optimizer=optimizer, loss=loss)
model_3.compile(optimizer=optimizer, loss=loss)

# model(x) ===> error. x should be in shape=(1,)
"""
Point to be noted here:

1. It is TRUE that using subclass API we no need to convert numpy to tensors for model.fit(), model.evaluate(), model.predict()
2. But when making the build/ input_shape we need to pass tensors, not numpys

"""
# Since we are using numpy arrays, we need to pass the build specifically
print(x.shape, y_true.shape, y_noise.shape)

x_reshaped = x.reshape(-1,1)
y_true_reshaped = y_true.reshape(-1,1)
y_noise_reshaped = y_noise.reshape(-1,1)

# Plot befire training
plt.scatter(x, y_noise, c="red", label="noisy data")
plt.plot(x, y_true, c="blue", label="true data")
plt.plot(x, model(x_reshaped), c="green", label="model prediction")
plt.legend()
plt.show()

model(x_reshaped).shape

model.fit(x_reshaped, y_noise_reshaped, epochs=200, verbose=2)

# After training
plt.scatter(x, y_noise, c="red", label="noisy data")
plt.plot(x, y_true, c="blue", label="true data")
plt.plot(x, model(x_reshaped), c="green", label="model prediction")
plt.legend()
plt.show()